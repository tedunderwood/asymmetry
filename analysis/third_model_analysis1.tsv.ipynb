{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third model, first analysis\n",
    "\n",
    "This is our third pass at getting a model that includes all the volumes we need. It adds about 40 volumes in a British canon (nonusnorton), and more importantly does some manual correction of metadata in order to generate \"contrast sets\" that are dated as accurately as our hypothesis sets.\n",
    "\n",
    "However, I don't expect results to change very much from the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob, math, random\n",
    "from scipy.stats import pearsonr, zscore, ttest_ind\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get metadata\n",
    "\n",
    "In order to analyze the data we will need a metadata table that includes information about volumes, including inferred biographical information about the author (age, gender, nationality) and columns that assign volumes to particular \"hypothesis\" and \"contrast\" sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39784, 39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['allcopiesofwork', 'author', 'copiesin25yrs', 'earlyedition', 'imprint',\n",
       "       'inferreddate', 'lastname', 'latestcomp', 'nationality', 'isusa',\n",
       "       'actualgender', 'likelygender', 'title', 'authordate', 'birth', 'age',\n",
       "       'recordid', 'best1821_1900', 'best1900_1950', 'best1950_1990',\n",
       "       'anybest', 'best1821_1900contrast', 'best1900_1950contrast',\n",
       "       'best1950_1990contrast', 'reviewed1850_1950',\n",
       "       'reviewed1850_1950contrast', 'heath', 'heathcontrast', 'mostdiscussed',\n",
       "       'mostdiscussedcontrast', 'usnorton', 'usnortoncontrast', 'nonusnorton',\n",
       "       'nonusnortoncontrast', 'preregistered', 'preregisteredcontrast',\n",
       "       'reviewed1965_1990', 'reviewed1965_1990contrast', 'toremove'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv('../supplement3/thirdmastermeta.tsv', sep = '\\t', index_col = 'docid')\n",
    "print(meta.shape)\n",
    "meta.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data\n",
    "\n",
    "Now the data itself. This is broken into a number of \"summary files\" because the processing that produced it had to be distributed across a cluster. (The entropy calculation is done by comparing individual volumes to each other, and when you've got 40k vols, the number of cross-comparisons becomes fairly large.)\n",
    "\n",
    "So we first make a list of all the files we need ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../supp3results/supp3_12000summary.tsv',\n",
       " '../supp3results/supp3_38000summary.tsv',\n",
       " '../supp3results/supp3_16000summary.tsv',\n",
       " '../supp3results/supp3_0summary.tsv',\n",
       " '../supp3results/supp3_8000summary.tsv',\n",
       " '../supp3results/supp3_28000summary.tsv',\n",
       " '../supp3results/supp3_26000summary.tsv',\n",
       " '../supp3results/supp3_22000summary.tsv',\n",
       " '../supp3results/supp3_2000summary.tsv',\n",
       " '../supp3results/supp3_6000summary.tsv',\n",
       " '../supp3results/supp3_32000summary.tsv',\n",
       " '../supp3results/supp3_36000summary.tsv',\n",
       " '../supp3results/supp3_18000summary.tsv',\n",
       " '../supp3results/supp3_20000summary.tsv',\n",
       " '../supp3results/supp3_24000summary.tsv',\n",
       " '../supp3results/supp3_34000summary.tsv',\n",
       " '../supp3results/supp3_30000summary.tsv',\n",
       " '../supp3results/supp3_4000summary.tsv',\n",
       " '../supp3results/supp3_14000summary.tsv',\n",
       " '../supp3results/supp3_10000summary.tsv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = glob.glob('../supp3results/*summary.tsv')\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then loop across the list, reading them in ... and finally concatenate the data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39784, 27)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for p in paths:\n",
    "    df = pd.read_csv(p, sep = '\\t', index_col = 'docid')\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, verify_integrity = True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank god, the number of rows in metadata == number of rows in data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test hypotheses on raw data\n",
    "\n",
    "There are lots of ways we can test our hypotheses, but first let's start simple. We'll just identify volumes in the hypothesis set, and those in a contrast set (matched as closely as practical to the hypothesis set in genre, nationality, and date). The [matching strategy](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2943670/) is a way of addressing confounding variables without some of the slipperiness of a complex regression.\n",
    "\n",
    "Then we run a t test on resonance for each of these categories. \n",
    "\n",
    "I'm primarily going to measure resonance at the 25-year window, and use all the volumes. I consider the column ```resonance_1.0_25``` the most reliable, generalized test. But I will also test at the 5% level, to see how that differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testahypothesis(hypothesis_cat, data, meta):\n",
    "    contrast_cat = hypothesis_cat + 'contrast'\n",
    "    \n",
    "    hypodocs = meta.index[meta[hypothesis_cat] == 1].tolist()\n",
    "    contradocs = meta.index[meta[contrast_cat] == 1].tolist()\n",
    "    \n",
    "    testoverlap = set(hypodocs).intersection(set(contradocs))\n",
    "    print(len(testoverlap))\n",
    "    \n",
    "    columns = ['resonance_1.0_25', 'resonance_0.05_25']\n",
    "    \n",
    "    for col in columns:\n",
    "        hypothesis_data = data.loc[hypodocs, col]\n",
    "        hypothesis_data = hypothesis_data[~np.isnan(hypothesis_data)]\n",
    "        \n",
    "        contrast_data = data.loc[contradocs, col]\n",
    "        contrast_data = contrast_data[~np.isnan(contrast_data)]\n",
    "        \n",
    "        t, p = ttest_ind(hypothesis_data, contrast_data)\n",
    "        print(col, \"t-test\", t, p)\n",
    "        a = hypothesis_data\n",
    "        b = contrast_data\n",
    "        cohens_d = (mean(a) - mean(b)) / (sqrt((stdev(a) ** 2 + stdev(b) ** 2) / 2))\n",
    "        print(col, \"Cohen's d\", cohens_d)\n",
    "        print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best1821_1900\n",
      "1\n",
      "resonance_1.0_25 t-test 4.18207502217 3.90826157017e-05\n",
      "resonance_1.0_25 Cohen's d 0.507151089725551\n",
      "\n",
      "resonance_0.05_25 t-test 4.87986587255 1.81742839832e-06\n",
      "resonance_0.05_25 Cohen's d 0.5917706597464294\n",
      "\n",
      "\n",
      "best1900_1950\n",
      "0\n",
      "resonance_1.0_25 t-test 2.51399280017 0.0124984220009\n",
      "resonance_1.0_25 Cohen's d 0.29941218533573655\n",
      "\n",
      "resonance_0.05_25 t-test 1.97802429527 0.0489061447334\n",
      "resonance_0.05_25 Cohen's d 0.23557926532491685\n",
      "\n",
      "\n",
      "best1950_1990\n",
      "0\n",
      "resonance_1.0_25 t-test -1.05483724148 0.292564931885\n",
      "resonance_1.0_25 Cohen's d -0.13592742178518313\n",
      "\n",
      "resonance_0.05_25 t-test -1.79659252091 0.0736630212982\n",
      "resonance_0.05_25 Cohen's d -0.2313211297690837\n",
      "\n",
      "\n",
      "reviewed1850_1950\n",
      "0\n",
      "resonance_1.0_25 t-test 6.07823498208 1.65383650847e-09\n",
      "resonance_1.0_25 Cohen's d 0.3609528297449262\n",
      "\n",
      "resonance_0.05_25 t-test 7.28942164376 5.79881532588e-13\n",
      "resonance_0.05_25 Cohen's d 0.4316225323709787\n",
      "\n",
      "\n",
      "heath\n",
      "0\n",
      "resonance_1.0_25 t-test 0.697160189666 0.487627577336\n",
      "resonance_1.0_25 Cohen's d 0.15019955591315579\n",
      "\n",
      "resonance_0.05_25 t-test -0.481599836454 0.631342564395\n",
      "resonance_0.05_25 Cohen's d -0.10358240201877814\n",
      "\n",
      "\n",
      "mostdiscussed\n",
      "0\n",
      "resonance_1.0_25 t-test 1.24295916987 0.219458358849\n",
      "resonance_1.0_25 Cohen's d 0.3382906374781595\n",
      "\n",
      "resonance_0.05_25 t-test 2.21528308623 0.0311444079161\n",
      "resonance_0.05_25 Cohen's d 0.6029236885644174\n",
      "\n",
      "\n",
      "usnorton\n",
      "0\n",
      "resonance_1.0_25 t-test 0.284712736417 0.776419724898\n",
      "resonance_1.0_25 Cohen's d 0.05479299167067695\n",
      "\n",
      "resonance_0.05_25 t-test -0.276567936696 0.782650439554\n",
      "resonance_0.05_25 Cohen's d -0.053225524233503484\n",
      "\n",
      "\n",
      "nonusnorton\n",
      "0\n",
      "resonance_1.0_25 t-test 2.27306074909 0.0253247270778\n",
      "resonance_1.0_25 Cohen's d 0.46703484851296323\n",
      "\n",
      "resonance_0.05_25 t-test 2.77590279886 0.00665580619693\n",
      "resonance_0.05_25 Cohen's d 0.5714840876990259\n",
      "\n",
      "\n",
      "preregistered\n",
      "0\n",
      "resonance_1.0_25 t-test 2.70030242458 0.0104901774852\n",
      "resonance_1.0_25 Cohen's d 0.8760937935957426\n",
      "\n",
      "resonance_0.05_25 t-test 3.29121332536 0.00223952884324\n",
      "resonance_0.05_25 Cohen's d 1.067810605769049\n",
      "\n",
      "\n",
      "reviewed1965_1990\n",
      "0\n",
      "resonance_1.0_25 t-test 2.4754180827 0.0144675735823\n",
      "resonance_1.0_25 Cohen's d 0.40960004416286233\n",
      "\n",
      "resonance_0.05_25 t-test 1.15813532551 0.248726007314\n",
      "resonance_0.05_25 Cohen's d 0.1916615513993298\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols2check = ['best1821_1900',\n",
    "       'best1900_1950', 'best1950_1990', 'reviewed1850_1950', 'heath', \n",
    "        'mostdiscussed', 'usnorton', 'nonusnorton', 'preregistered',\n",
    "       'reviewed1965_1990']\n",
    "\n",
    "for col in cols2check:\n",
    "    print(col)\n",
    "    testahypothesis(col, data, meta)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis\n",
    "\n",
    "Everything above makes sense.\n",
    "\n",
    "1. Clear significant positive effects: reviewed books both 1850-1950 and 1965-1990. Our preregistered 20 vols, also the non-US Norton vols. Bestsellers *until* 1950.\n",
    "\n",
    "2. Heath, usnorton, and mostdiscussed are not significant effects. For all of those groups, n is pretty small. On the other hand, nonusnorton and preregistered are significant with a sample size that's no larger, so the negative result here seems meaningful.\n",
    "\n",
    "3. The declining significance of bestsellerdom is pretty clear.\n",
    "\n",
    "There's also one error in there: the \"1\" instead of a \"0\" in best1821_1900 suggests that we have a tiny overlap between the hypothesis and contrast set. That shouldn't happen. I will set about fixing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
