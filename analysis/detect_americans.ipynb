{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Americans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading some ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago = pd.read_csv('/Users/tunder/Dropbox/CHICAGO_CORPUS/CHICAGO_NOVEL_CORPUS_METADATA/CHICAGO_CORPUS_AUTHORS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalities = dict()\n",
    "\n",
    "for idx, row in chicago.iterrows():\n",
    "    name = row['AUTH_LAST'] + ', ' + row['AUTH_FIRST']\n",
    "    \n",
    "    if row['NATIONALITY'].lower().startswith('ameri'):\n",
    "        nation = 'us'\n",
    "    else:\n",
    "        nation = 'non'\n",
    "    \n",
    "    nationalities[name] = nation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationalities['Koontz, Dean R.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = pd.read_csv('../../metascrape/authornationalities.csv')\n",
    "an = an.rename(columns = {'name': 'author'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2nations(df):\n",
    "    global nationalities\n",
    "    for idx, row in df.iterrows():\n",
    "        if not pd.isnull(row['nationality']) and not pd.isnull(row['author']):\n",
    "            nationalities[row['author']] = row['nationality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4820\n"
     ]
    }
   ],
   "source": [
    "add2nations(an)\n",
    "print(len(nationalities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5382\n"
     ]
    }
   ],
   "source": [
    "p = pd.read_csv('../../meta2018/patrick.tsv')\n",
    "add2nations(p)\n",
    "print(len(nationalities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5957\n"
     ]
    }
   ],
   "source": [
    "j = pd.read_csv('../../meta2018/jessica.tsv')\n",
    "add2nations(j)\n",
    "print(len(nationalities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6549\n"
     ]
    }
   ],
   "source": [
    "ssm = pd.read_csv('../supplement2/second_supplement_refined.tsv', sep = '\\t')\n",
    "add2nations(ssm)\n",
    "print(len(nationalities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = dict()\n",
    "\n",
    "for author, nation in nationalities.items():\n",
    "    if len(author) < 2:\n",
    "        code = 'xx'\n",
    "    else:\n",
    "        code = author.lower()[0:2]\n",
    "        \n",
    "    if code not in blocks:\n",
    "        blocks[code] = []\n",
    "    blocks[code].append(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = dict()\n",
    "supp2 = pd.read_csv('../supplement2/supp2allmeta.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "101\n",
      "201\n",
      "301\n",
      "401\n",
      "501\n",
      "601\n",
      "701\n",
      "801\n",
      "901\n",
      "1001\n",
      "1101\n",
      "1201\n",
      "1301\n",
      "1401\n",
      "1501\n",
      "1601\n",
      "1701\n",
      "1801\n",
      "1901\n",
      "2001\n",
      "2101\n",
      "2201\n",
      "2301\n",
      "2401\n",
      "2501\n",
      "2601\n",
      "2701\n",
      "2801\n",
      "2901\n",
      "3001\n",
      "3101\n",
      "3201\n",
      "3301\n",
      "3401\n",
      "3501\n",
      "3601\n",
      "3701\n",
      "3801\n",
      "3901\n",
      "4001\n",
      "4101\n",
      "4201\n",
      "4301\n",
      "4401\n",
      "4501\n",
      "4601\n",
      "4701\n",
      "4801\n",
      "4901\n",
      "5001\n",
      "5101\n",
      "5201\n",
      "5301\n",
      "5401\n",
      "5501\n",
      "5601\n",
      "5701\n",
      "5801\n",
      "5901\n",
      "6001\n",
      "6101\n",
      "6201\n",
      "6301\n",
      "6401\n",
      "6501\n",
      "6601\n",
      "6701\n",
      "6801\n",
      "6901\n",
      "7001\n",
      "7101\n",
      "7201\n",
      "7301\n",
      "7401\n",
      "7501\n",
      "7601\n",
      "7701\n",
      "7801\n",
      "7901\n",
      "8001\n",
      "8101\n",
      "8201\n",
      "8301\n",
      "8401\n",
      "8501\n",
      "8601\n",
      "8701\n",
      "8801\n",
      "8901\n",
      "9001\n",
      "9101\n",
      "9201\n",
      "9301\n",
      "9401\n",
      "9501\n",
      "9601\n",
      "9701\n",
      "9801\n",
      "9901\n",
      "10001\n",
      "10101\n",
      "10201\n",
      "10301\n",
      "10401\n",
      "10501\n",
      "10601\n",
      "10701\n",
      "10801\n",
      "10901\n",
      "11001\n",
      "11101\n",
      "11201\n",
      "11301\n",
      "11401\n",
      "11501\n",
      "11601\n",
      "11701\n",
      "11801\n",
      "11901\n",
      "12001\n",
      "12101\n",
      "12201\n",
      "12301\n",
      "12401\n",
      "12501\n",
      "12601\n",
      "12701\n",
      "12801\n",
      "12901\n",
      "13001\n",
      "13101\n",
      "13201\n",
      "13301\n",
      "13401\n",
      "13501\n",
      "13601\n",
      "13701\n",
      "13801\n",
      "13901\n",
      "14001\n",
      "14101\n",
      "14201\n",
      "14301\n",
      "14401\n",
      "14501\n",
      "14601\n",
      "14701\n",
      "14801\n",
      "14901\n",
      "15001\n",
      "15101\n",
      "15201\n",
      "15301\n",
      "15401\n",
      "15501\n",
      "15601\n",
      "15701\n",
      "15801\n",
      "15901\n",
      "16001\n",
      "16101\n",
      "16201\n",
      "16301\n",
      "16401\n",
      "16501\n",
      "16601\n",
      "16701\n",
      "16801\n",
      "16901\n",
      "17001\n",
      "17101\n",
      "17201\n",
      "17301\n",
      "17401\n",
      "17501\n",
      "17601\n",
      "17701\n",
      "17801\n",
      "17901\n",
      "18001\n",
      "18101\n",
      "18201\n",
      "18301\n",
      "18401\n",
      "18501\n",
      "18601\n",
      "18701\n",
      "18801\n",
      "18901\n",
      "19001\n",
      "19101\n",
      "19201\n",
      "19301\n",
      "19401\n",
      "19501\n",
      "19601\n",
      "19701\n",
      "19801\n",
      "19901\n",
      "20001\n",
      "20101\n",
      "20201\n",
      "20301\n",
      "20401\n",
      "20501\n",
      "20601\n",
      "20701\n",
      "20801\n",
      "20901\n",
      "21001\n",
      "21101\n",
      "21201\n",
      "21301\n",
      "21401\n",
      "21501\n",
      "21601\n",
      "21701\n",
      "21801\n",
      "21901\n",
      "22001\n",
      "22101\n",
      "22201\n",
      "22301\n",
      "22401\n",
      "22501\n",
      "22601\n",
      "22701\n",
      "22801\n",
      "22901\n",
      "23001\n",
      "23101\n",
      "23201\n",
      "23301\n",
      "23401\n",
      "23501\n",
      "23601\n",
      "23701\n",
      "23801\n",
      "23901\n",
      "24001\n",
      "24101\n",
      "24201\n",
      "24301\n",
      "24401\n",
      "24501\n",
      "24601\n",
      "24701\n",
      "24801\n",
      "24901\n",
      "25001\n",
      "25101\n",
      "25201\n",
      "25301\n",
      "25401\n",
      "25501\n",
      "25601\n",
      "25701\n",
      "25801\n",
      "25901\n",
      "26001\n",
      "26101\n",
      "26201\n",
      "26301\n",
      "26401\n",
      "26501\n",
      "26601\n",
      "26701\n",
      "26801\n",
      "26901\n",
      "27001\n",
      "27101\n",
      "27201\n",
      "27301\n",
      "27401\n",
      "27501\n",
      "27601\n",
      "27701\n",
      "27801\n",
      "27901\n",
      "28001\n",
      "28101\n",
      "28201\n",
      "28301\n",
      "28401\n",
      "28501\n",
      "28601\n",
      "28701\n",
      "28801\n",
      "28901\n",
      "29001\n",
      "29101\n",
      "29201\n",
      "29301\n",
      "29401\n",
      "29501\n",
      "29601\n",
      "29701\n",
      "29801\n",
      "29901\n",
      "30001\n",
      "30101\n",
      "30201\n",
      "30301\n",
      "30401\n",
      "30501\n",
      "30601\n",
      "30701\n",
      "30801\n",
      "30901\n",
      "31001\n",
      "31101\n",
      "31201\n",
      "31301\n",
      "31401\n",
      "31501\n",
      "31601\n",
      "31701\n",
      "31801\n",
      "31901\n",
      "32001\n",
      "32101\n",
      "32201\n",
      "32301\n",
      "32401\n",
      "32501\n",
      "32601\n",
      "32701\n",
      "32801\n",
      "32901\n",
      "33001\n",
      "33101\n",
      "33201\n",
      "33301\n",
      "33401\n",
      "33501\n",
      "33601\n",
      "33701\n",
      "33801\n",
      "33901\n",
      "34001\n",
      "34101\n",
      "34201\n",
      "34301\n",
      "34401\n",
      "34501\n",
      "34601\n",
      "34701\n",
      "34801\n",
      "34901\n",
      "35001\n",
      "35101\n",
      "35201\n",
      "35301\n",
      "35401\n",
      "35501\n",
      "35601\n",
      "35701\n",
      "35801\n",
      "35901\n",
      "36001\n",
      "36101\n",
      "36201\n",
      "36301\n",
      "36401\n",
      "36501\n",
      "36601\n",
      "36701\n",
      "36801\n",
      "36901\n",
      "37001\n",
      "37101\n",
      "37201\n",
      "37301\n",
      "37401\n",
      "37501\n",
      "37601\n",
      "37701\n",
      "37801\n",
      "37901\n",
      "38001\n",
      "38101\n",
      "38201\n",
      "38301\n",
      "38401\n",
      "38501\n",
      "38601\n",
      "38701\n",
      "38801\n",
      "38901\n",
      "39001\n",
      "39101\n",
      "39201\n",
      "39301\n",
      "39401\n",
      "39501\n",
      "39601\n",
      "39701\n",
      "39801\n",
      "3637\n"
     ]
    }
   ],
   "source": [
    "def fuzzymatch(str1, str2):\n",
    "    \n",
    "    m = SequenceMatcher(None, str1, str2)\n",
    "    match = m.real_quick_ratio()\n",
    "    if match > 0.7:\n",
    "        match = m.ratio()\n",
    "    \n",
    "    return match\n",
    "\n",
    "def trim(astring):\n",
    "    astring = astring.strip('[]().,')\n",
    "    if '(' in astring:\n",
    "        return astring.split('(')[0]\n",
    "    else:\n",
    "        return astring\n",
    "    \n",
    "ctr = 0\n",
    "for a in supp2.author:\n",
    "    ctr += 1\n",
    "    if ctr % 100 == 1:\n",
    "        print(ctr)\n",
    "    if pd.isnull(a):\n",
    "        continue\n",
    "    matches = []\n",
    "    trimmed = trim(a)\n",
    "    if a in mapped:\n",
    "        continue\n",
    "    elif len(a) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        code = a.lower()[0: 2]\n",
    "        if code not in blocks:\n",
    "            continue\n",
    "        block = blocks[code]\n",
    "        for a2 in block:\n",
    "            trim2 = trim(a2)\n",
    "            sim = fuzzymatch(a, a2)\n",
    "            sim2 = fuzzymatch(trimmed, trim2)\n",
    "            similarity = max(sim, sim2)\n",
    "            if similarity > 0.9:\n",
    "                matches.append((similarity, a2))\n",
    "    \n",
    "    if len(matches) > 0:\n",
    "        matches.sort()\n",
    "        closest = matches[-1][1]\n",
    "        mapped[a] = closest\n",
    "\n",
    "print(len(mapped))\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usethemap(author):\n",
    "    global mapped, nationalities\n",
    "    if pd.isnull(author):\n",
    "        return float('nan')\n",
    "    elif author in mapped:\n",
    "        return nationalities[mapped[author]]\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "supp2 = supp2.assign(nationality = supp2.author.map(usethemap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6369\n"
     ]
    }
   ],
   "source": [
    "print(sum(supp2.nationality == 'us'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New York', 13786),\n",
       " ('London', 11817),\n",
       " ('Boston', 2712),\n",
       " ('Philadelphia', 1539),\n",
       " ('Edinburgh', 619),\n",
       " ('Garden City, N.Y.', 619),\n",
       " ('Chicago', 467),\n",
       " ('Leipzig', 380),\n",
       " ('Indianapolis', 223),\n",
       " ('Toronto', 206),\n",
       " ('New Delhi', 191),\n",
       " ('New York, N.Y.', 170),\n",
       " ('Garden City, N. Y.', 140),\n",
       " ('San Francisco', 128),\n",
       " ('New-York', 128),\n",
       " ('Dublin', 115),\n",
       " ('Sydney', 113),\n",
       " ('Moscow', 111),\n",
       " ('Paris', 80),\n",
       " ('Cincinnati', 74),\n",
       " ('New York and London', 70),\n",
       " ('Garden City, New York', 65),\n",
       " ('Baltimore', 59),\n",
       " ('New York, NY', 57),\n",
       " ('Calcutta', 51),\n",
       " ('S.l.', 50),\n",
       " ('New York [etc.', 50),\n",
       " ('Los Angeles', 47),\n",
       " ('Bombay', 47),\n",
       " ('New York, N.Y., U.S.A.', 46),\n",
       " ('Glasgow', 45),\n",
       " ('Delhi', 44),\n",
       " ('Cleveland', 44),\n",
       " ('Singapore', 44),\n",
       " ('Melbourne', 44),\n",
       " ('Nairobi', 43),\n",
       " ('Oxford', 42),\n",
       " ('Boston, Mass.', 38),\n",
       " ('Quezon City', 38),\n",
       " ('Washington', 36),\n",
       " ('London [etc.', 36),\n",
       " ('Chapel Hill, N.C.', 35),\n",
       " ('Pleasantville, N.Y.', 33),\n",
       " ('Lincoln', 32),\n",
       " ('Cape Town', 32),\n",
       " ('Tokyo', 32),\n",
       " ('Colombo', 31),\n",
       " ('Boston and New York', 30),\n",
       " ('Minneapolis', 30),\n",
       " ('Richmond', 29),\n",
       " ('Washington, D.C.', 28),\n",
       " ('London, New York', 27),\n",
       " ('Montreal', 27),\n",
       " ('Manila', 26),\n",
       " ('Hartford, Conn.', 25),\n",
       " ('Manchester', 25),\n",
       " ('Peking', 24),\n",
       " ('N.Y.', 24),\n",
       " ('Johannesburg', 24),\n",
       " ('Tallahassee, Fla.', 23),\n",
       " ('Los Angeles, Calif.', 23),\n",
       " ('San Diego', 23),\n",
       " ('Cambridge', 23),\n",
       " ('Nashville, Tenn.', 23),\n",
       " ('Evanston, Ill.', 22),\n",
       " ('Harmondsworth', 22),\n",
       " ('Hartford', 22),\n",
       " ('Ottawa', 21),\n",
       " ('Providence', 20),\n",
       " ('Cambridge, Mass.', 20),\n",
       " ('Columbia', 19),\n",
       " ('Urbana', 19),\n",
       " ('Pittsburgh, Pa.', 18),\n",
       " ('Ibadan', 18),\n",
       " ('s.l.', 17),\n",
       " ('Bristol', 17),\n",
       " ('Berlin', 17),\n",
       " ('Cairo', 16),\n",
       " ('Madras', 16),\n",
       " ('Vancouver', 16),\n",
       " ('Albuquerque', 16),\n",
       " ('Garden City', 16),\n",
       " ('Chicago, Ill.', 15),\n",
       " ('Minneapolis, Minn.', 15),\n",
       " ('Albany', 15),\n",
       " ('Santee, Calif.', 15),\n",
       " ('Austin', 15),\n",
       " ('Greenwich, Conn.', 14),\n",
       " ('Minneapolis, MN', 14),\n",
       " ('Santa Barbara', 14),\n",
       " ('St. Louis', 14),\n",
       " ('Bangkok', 14),\n",
       " ('Berkeley', 14),\n",
       " ('Milwaukee', 14),\n",
       " ('Englewood Cliffs, N.J.', 14),\n",
       " ('Iowa City', 13),\n",
       " ('Richmond, Va.', 13),\n",
       " ('Westminster', 13),\n",
       " ('Belfast', 13),\n",
       " ('Berkeley, Calif.', 13)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = Counter()\n",
    "for imprint in supp2.imprint:\n",
    "    if pd.isnull(imprint):\n",
    "        continue\n",
    "    parts = imprint.split('|')\n",
    "    city = parts[0]\n",
    "    if ';' in city:\n",
    "        city = city.split(';')[0]\n",
    "    cities[city] += 1\n",
    "\n",
    "cities.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "yankeecities = {'New York', 'Boston', 'Philadelphia', 'Garden City, N.Y.', 'Chicago', \n",
    "           'Indianapolis', 'New York, N.Y.', 'Garden City, N. Y.',\n",
    "           'San Francisco', 'New-York', 'Cincinnati', 'Garden City, New York',\n",
    "           'Baltimore', 'New York, NY', 'Los Angeles', 'New York, N.Y., U.S.A.',\n",
    "           'Cleveland', 'Pleasantville, N.Y.', 'Washington', 'Boston and New York',\n",
    "           'Boston, Mass.', 'Washington, D.C.', 'Tallahassee, Fla.',\n",
    "           'N.Y.', 'Hartford, Conn.', 'Los Angeles, Calif.', 'San Diego',\n",
    "               'Evanston, Ill.', 'Hartford', 'Cambridge, Mass.', 'Providence', 'Pittsburgh, Pa.',\n",
    "               'Chicago, Ill.', 'Minneapolis, Minn.', 'Albany', 'Santa Barbara', 'St. Louis',\n",
    "               'Berkeley', 'Englewood Cliffs, N.J.', 'Iowa City', 'Richmond, Va.'}\n",
    "\n",
    "localus = {'Indianapolis', 'San Francisco', 'Cincinnati',\n",
    "           'Baltimore', 'Los Angeles',\n",
    "           'Cleveland', 'Pleasantville, N.Y.', 'Washington', 'Washington, D.C.', \n",
    "           'Tallahassee, Fla.', 'Hartford, Conn.', 'Los Angeles, Calif.', 'San Diego',\n",
    "               'Evanston, Ill.', 'Hartford', 'Cambridge, Mass.', 'Providence', 'Pittsburgh, Pa.',\n",
    "               'Chicago, Ill.', 'Minneapolis, Minn.', 'Albany', 'Santa Barbara', 'St. Louis',\n",
    "               'Berkeley', 'Englewood Cliffs, N.J.', 'Iowa City', 'Richmond, Va.'}\n",
    "\n",
    "localnonus = {'Edinburgh', 'New Delhi', 'Sydney', 'Dublin', 'Nairobi', 'Moscow',\n",
    "             'Paris', 'Calcutta', 'Bombay', 'Glasgow', 'Delhi', 'Melbourne', 'Cape Town'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorgroups = supp2.groupby('author')\n",
    "ratios = []\n",
    "numbernot = []\n",
    "numberyes = []\n",
    "dates = []\n",
    "nations = []\n",
    "kansas = []\n",
    "notinkansas = []\n",
    "\n",
    "for author, df in authorgroups:\n",
    "    if author not in mapped:\n",
    "        continue\n",
    "    else:\n",
    "        newctr = 0\n",
    "        kansasctr = 0\n",
    "        notinkansasctr = 0\n",
    "        for imprint in df.imprint:\n",
    "            if pd.isnull(imprint):\n",
    "                continue\n",
    "            parts = imprint.split('|')\n",
    "            city = parts[0]\n",
    "            if ';' in city:\n",
    "                city = city.split(';')[0]\n",
    "                \n",
    "            if city in yankeecities:\n",
    "                newctr += 1\n",
    "            if city in localus:\n",
    "                kansasctr += 1\n",
    "            if city in localnonus:\n",
    "                notinkansasctr += 1\n",
    "        totalvols = len(df.imprint)\n",
    "        ratio = newctr / totalvols\n",
    "        \n",
    "        nation = nationalities[mapped[author]]\n",
    "        if nation == 'us':\n",
    "            nations.append(1)\n",
    "        else:\n",
    "            nations.append(0)\n",
    "        \n",
    "        ratios.append(ratio)\n",
    "        dates.append(np.mean(df.latestcomp))\n",
    "        numberyes.append(newctr)\n",
    "        numbernot.append(len(df.imprint) - newctr)\n",
    "        kansas.append(kansasctr)\n",
    "        notinkansas.append((notinkansasctr + .01) / totalvols)\n",
    "        \n",
    "        \n",
    "X = pd.DataFrame({'pubplaces': ratios, 'latestcomp': dates, 'not': numbernot, \n",
    "                  'yes': numberyes, 'kansas': kansas, 'notinks': notinkansas})\n",
    "y = np.array(nations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.469425\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 3637\n",
      "Model:                          Logit   Df Residuals:                     3631\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Sat, 14 Jul 2018   Pseudo R-squ.:                  0.3176\n",
      "Time:                        14:03:10   Log-Likelihood:                -1707.3\n",
      "converged:                       True   LL-Null:                       -2502.0\n",
      "                                        LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "kansas         0.8334      0.217      3.848      0.000       0.409       1.258\n",
      "latestcomp -1.691e-05   6.25e-05     -0.270      0.787      -0.000       0.000\n",
      "not           -0.6418      0.051    -12.486      0.000      -0.743      -0.541\n",
      "notinks       -1.1209      0.464     -2.418      0.016      -2.030      -0.212\n",
      "pubplaces      1.2459      0.149      8.356      0.000       0.954       1.538\n",
      "yes            0.1074      0.015      7.022      0.000       0.077       0.137\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.335009\n",
       "1    0.335024\n",
       "2    0.334994\n",
       "3    0.787403\n",
       "4    0.025506\n",
       "5    0.787440\n",
       "6    0.334893\n",
       "7    0.787304\n",
       "8    0.335284\n",
       "9    0.679127\n",
       "dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = result.predict(X)\n",
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7954357987352213"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gotright = 0\n",
    "gotwrong = 0\n",
    "\n",
    "for p, r in zip(predictions, y):\n",
    "    if p > 0.5 and r > 0.5:\n",
    "        gotright += 1\n",
    "    elif p < 0.5 and r < 0.5:\n",
    "        gotright += 1\n",
    "    else:\n",
    "        gotwrong += 1\n",
    "\n",
    "gotright / (gotright + gotwrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965240977471933"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((39817 - 16577) * 0.205) / 39817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23240, 12)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched = supp2.loc[pd.isnull(supp2.nationality), : ]\n",
    "unmatched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunder/miniconda3/lib/python3.5/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "ctr = 1\n",
    "for idx, row in unmatched.iterrows():\n",
    "    if pd.isnull(row['author']) or len(row['author']) < 3:\n",
    "        unmatched.loc[idx, 'author'] = 'anonym' + str(ctr)\n",
    "        ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17705\n"
     ]
    }
   ],
   "source": [
    "missingauths = unmatched.groupby('author')\n",
    "print(len(missingauths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = []\n",
    "numbernot = []\n",
    "numberyes = []\n",
    "dates = []\n",
    "kansas = []\n",
    "notinkansas = []\n",
    "themissingauthors = []\n",
    "\n",
    "for author, df in missingauths:\n",
    "    newctr = 0\n",
    "    kansasctr = 0\n",
    "    notinkansasctr = 0\n",
    "    for imprint in df.imprint:\n",
    "        if pd.isnull(imprint):\n",
    "            continue\n",
    "        parts = imprint.split('|')\n",
    "        city = parts[0]\n",
    "        if ';' in city:\n",
    "            city = city.split(';')[0]\n",
    "\n",
    "        if city in yankeecities:\n",
    "            newctr += 1\n",
    "        if city in localus:\n",
    "            kansasctr += 1\n",
    "        if city in localnonus:\n",
    "            notinkansasctr += 1\n",
    "    totalvols = len(df.imprint)\n",
    "    ratio = newctr / totalvols\n",
    "\n",
    "    ratios.append(ratio)\n",
    "    dates.append(np.mean(df.latestcomp))\n",
    "    numberyes.append(newctr)\n",
    "    numbernot.append(len(df.imprint) - newctr)\n",
    "    kansas.append(kansasctr)\n",
    "    notinkansas.append((notinkansasctr + .01) / totalvols)\n",
    "    themissingauthors.append(author)\n",
    "        \n",
    "        \n",
    "X = pd.DataFrame({'pubplaces': ratios, 'latestcomp': dates, 'not': numbernot, \n",
    "                  'yes': numberyes, 'kansas': kansas, 'notinks': notinkansas})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpredictions = result.predict(X)\n",
    "X = X.assign(pred = newpredictions)\n",
    "X = X.assign(auth = themissingauthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kansas</th>\n",
       "      <th>latestcomp</th>\n",
       "      <th>not</th>\n",
       "      <th>notinks</th>\n",
       "      <th>pubplaces</th>\n",
       "      <th>yes</th>\n",
       "      <th>pred</th>\n",
       "      <th>auth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335371</td>\n",
       "      <td>(Goodwin, William)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140886</td>\n",
       "      <td>&amp;#xd6;stergren, Klas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1937.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334968</td>\n",
       "      <td>'Edith', pseud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1824.428571</td>\n",
       "      <td>6</td>\n",
       "      <td>0.287143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019486</td>\n",
       "      <td>, Lady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334757</td>\n",
       "      <td>1947- Tan, Mark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kansas   latestcomp  not   notinks  pubplaces  yes      pred  \\\n",
       "0       0  1830.000000    1  0.010000   0.000000    0  0.335371   \n",
       "1       0  2009.000000    1  1.010000   0.000000    0  0.140886   \n",
       "2       0  1937.000000    1  0.010000   0.000000    0  0.334968   \n",
       "3       0  1824.428571    6  0.287143   0.142857    1  0.019486   \n",
       "4       0  1993.000000    1  0.010000   0.000000    0  0.334757   \n",
       "\n",
       "                   auth  \n",
       "0    (Goodwin, William)  \n",
       "1  &#xd6;stergren, Klas  \n",
       "2        'Edith', pseud  \n",
       "3                , Lady  \n",
       "4       1947- Tan, Mark  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapthemissing = dict()\n",
    "for idx, row in X.iterrows():\n",
    "    if row['pred'] > 0.5:\n",
    "        mapthemissing[row['auth']] = 'guess: us'\n",
    "    else:\n",
    "        mapthemissing[row['auth']] = 'guess: non-us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gore, (Catherine Grace Frances), Mrs 63 guess: non-us\n",
      "Riddell, J. H., Mrs 32 guess: non-us\n",
      "Marryat, Florence R. M. Church Lean 31 guess: non-us\n",
      "Ward, R. Plumer (Robert Plumer) 25 guess: non-us\n",
      "Lee, Holme 22 guess: non-us\n",
      "Carlyle, Thomas 22 guess: non-us\n",
      "Grey, (Elizabeth Caroline), Mrs 20 guess: non-us\n",
      "Countess of, Blessington, Marguerite 19 guess: non-us\n",
      "Hanley, James 18 guess: non-us\n",
      "Zola, Émile 18 guess: non-us\n",
      "Chatterton, Georgiana, Lady 17 guess: non-us\n",
      "Aimard, Gustave 17 guess: non-us\n",
      "Haliburton, Thomas Chandler 17 guess: non-us\n",
      "Bray 16 guess: non-us\n",
      "Charlotte Elizabeth 16 guess: us\n",
      "M'Donogh, Felix 15 guess: non-us\n",
      "Yerby, Frank 14 guess: us\n",
      "Andersen, H. C. (Hans Christian) 14 guess: us\n",
      "Morgan 14 guess: non-us\n",
      "Sienkiewicz, Henryk 14 guess: us\n",
      "Mühlbach, L. (Luise) 13 guess: us\n",
      "Praed, Campbell, Mrs 13 guess: non-us\n",
      "Lauder, Thomas Dick, Sir 13 guess: non-us\n",
      "Hentz, Caroline Lee 13 guess: us\n",
      "Jókai, Mór 13 guess: non-us\n",
      "Freiherr de, La Motte-Fouqué, Friedrich Heinrich Karl 12 guess: non-us\n",
      "Madame Cottin, (Sophie) 12 guess: non-us\n",
      "M?_hlbach, L 11 guess: us\n",
      "Davies, Rhys 11 guess: non-us\n",
      "Lockhart, J. G. (John Gibson) 11 guess: non-us\n",
      "Neale, W. Johnson (William Johnson) 10 guess: non-us\n",
      "Feuchtwanger, Lion 10 guess: us\n",
      "Trevor, William 10 guess: non-us\n",
      "Homer 10 guess: non-us\n",
      "Normanby, Constantine Henry Phipps 10 guess: non-us\n",
      "Macaulay, Rose, Dame 10 guess: non-us\n",
      "Mann, Thomas 10 guess: non-us\n",
      "Hewlett, Maurice Henry 10 guess: non-us\n",
      "Flaubert, Gustave 10 guess: non-us\n",
      "Saroyan, William 10 guess: us\n",
      "McIntosh, Maria J. (Maria Jane) 9 guess: us\n",
      "Becke, Louis 9 guess: non-us\n",
      "Marlitt, E. (Eugenie) 9 guess: us\n",
      "Chaucer, Geoffrey 9 guess: non-us\n",
      "Vachell, Horace Annesley 9 guess: non-us\n",
      "Brown, Thomas 9 guess: non-us\n",
      "Burke, Thomas 9 guess: us\n",
      "Newby, P. H. (Percy Howard) 9 guess: non-us\n",
      "Grant, Maria M 9 guess: non-us\n",
      "Boccaccio, Giovanni 9 guess: non-us\n",
      "Coolidge, Dane 9 guess: us\n",
      "Ertz, Susan 8 guess: us\n",
      "La Harpe, Jean-François de 8 guess: non-us\n",
      "Maḥfūẓ, Najīb 8 guess: non-us\n",
      "Ward, Catherine G. (Catherine George) 8 guess: non-us\n",
      "Lucas, E. V. (Edward Verrall) 8 guess: non-us\n",
      "Thomes, William Henry 8 guess: us\n",
      "Boyle, Kay 8 guess: us\n",
      "Sagan, Françoise 8 guess: non-us\n",
      "Kennedy, Margaret 8 guess: us\n",
      "Malory, Thomas, Sir 8 guess: non-us\n",
      "Hervey, Elizabeth 8 guess: non-us\n",
      "Sargent, Lucius M. (Lucius Manlius) 8 guess: us\n",
      "Palmer, Vance 8 guess: non-us\n",
      "Bourget, Paul 8 guess: us\n",
      "Tracy, Honor 8 guess: us\n",
      "Lamb, Caroline 8 guess: non-us\n",
      "Bacheller, Irving Addison 8 guess: us\n",
      "Smythies, Gordon, Mrs 8 guess: non-us\n",
      "Callaghan, Morley 8 guess: non-us\n",
      "Powys, Theodore Francis 8 guess: non-us\n",
      "Diver, Maud 8 guess: us\n",
      "Waddington, Julia Rattray 7 guess: non-us\n",
      "Irwin, Wallace 7 guess: non-us\n",
      "Blish, James 7 guess: us\n",
      ", Lady 7 guess: non-us\n",
      "Miller, J. (Joaquin) 7 guess: us\n",
      "Bindloss, Harold 7 guess: us\n",
      "Ross, Mrs 7 guess: non-us\n",
      "Shaw, Bernard 7 guess: us\n",
      "Betham-Edwards, Matilda 7 guess: non-us\n",
      "Neumann, Alfred 7 guess: us\n",
      "Savage, M. W 7 guess: non-us\n",
      "Mackenzie, Mary Jane 7 guess: non-us\n",
      "Agg, John 7 guess: non-us\n",
      "Maeterlinck, Maurice 7 guess: us\n",
      "Eberhart, Mignon (Good), Mrs 7 guess: us\n",
      "Scott, Evelyn, Mrs 7 guess: us\n",
      "Tennant, Kylie 7 guess: non-us\n",
      "Williams, Robert Folkestone 7 guess: non-us\n",
      "Berkeley, Grantley F. (Grantley Fitzhardinge) 7 guess: non-us\n",
      "Cary, Joyce 7 guess: non-us\n",
      "comtesse de, Genlis, Stéphanie Félicité 7 guess: non-us\n",
      "Gerard, E. (Emily) 7 guess: non-us\n",
      "McCarthy, Justin H , (Justin Huntly) 7 guess: non-us\n",
      "Howell, Thomas Bayly 7 guess: non-us\n",
      "Marks, Percy 7 guess: us\n",
      "Le Carré, John 7 guess: us\n",
      "Sansom, William 7 guess: non-us\n",
      "Treece, Henry 7 guess: non-us\n"
     ]
    }
   ],
   "source": [
    "biggest_missing = Counter()\n",
    "for name, df in missingauths:\n",
    "    biggest_missing[name] = len(df.author)\n",
    "bigs = biggest_missing.most_common(100)\n",
    "for aname, acount in bigs:\n",
    "    print(aname, acount, mapthemissing[aname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapthemissing['Mühlbach, L. (Luise)'] = 'de'\n",
    "mapthemissing['M?_hlbach, L'] = 'de'\n",
    "mapthemissing['Andersen, H. C. (Hans Christian)'] = 'da'\n",
    "mapthemissing['Sienkiewicz, Henryk'] = 'po'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid2nation = dict()\n",
    "\n",
    "for idx, row in unmatched.iterrows():\n",
    "    auth = row['author']\n",
    "    if auth not in mapthemissing:\n",
    "        print('Danger, Will Robinson.')\n",
    "    else:\n",
    "        docid2nation[row['docid']] = mapthemissing[auth]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in supp2.iterrows():\n",
    "    if not pd.isnull(row['nationality']):\n",
    "        continue\n",
    "    elif row['docid'] not in docid2nation:\n",
    "        print('Danger!')\n",
    "    else:\n",
    "        supp2.loc[idx, 'nationality'] = docid2nation[row['docid']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pd.isnull(supp2.nationality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp2.to_csv('../supplement2/supp2nationalitymeta.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
